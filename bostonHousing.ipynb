{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eca0fef0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d517ff64",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c71961a2",
   "metadata": {},
   "source": [
    "Let’s dive into building a machine learning project with the **Boston Housing Dataset!** I’ll guide you step-by-step as an experienced AI/ML engineer, from downloading the dataset to applying advanced techniques. We’ll predict median house prices in Boston suburbs using regression, covering data preprocessing, exploratory data analysis (EDA), feature engineering, model building, evaluation, and advanced methods like hyperparameter tuning and interpretability. Let’s get started!\n",
    "## Project Overview\n",
    "**Goal:** Predict median house prices (MEDV) in $1000s based on features like crime rate, number of rooms, and more.\n",
    "\n",
    "**Dataset:** Boston Housing Dataset, available via Scikit-learn.\n",
    "\n",
    "**Tools:** Python, Pandas, NumPy, Scikit-learn, Matplotlib, Seaborn, and optionally XGBoost.\n",
    "\n",
    "**Skills:** Data preprocessing, EDA, feature engineering, model selection, tuning, and interpretability.\n",
    "\n",
    "#### Step 1: Download and Load the Dataset\n",
    "The Boston Housing Dataset is built into Scikit-learn, so downloading is straightforward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0629a063",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "boston = load_boston()\n",
    "X = pd.DataFrame(boston.data, columns=boston.feature_names)  # Features\n",
    "y = pd.Series(boston.target, name='MEDV')  # Target: Median house price\n",
    "\n",
    "# Combine into one DataFrame\n",
    "df = pd.concat([X, y], axis=1)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3a0de1",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "**Features (X):** 13 numerical columns, e.g., CRIM (crime rate), RM (average rooms), LSTAT (lower status population %).\n",
    "\n",
    "**Target (y):** MEDV, the median house price in $1000s.\n",
    "\n",
    "**Output:** Displays the first 5 rows to confirm loading.\n",
    "\n",
    "### Step 2: Data Cleaning and Preprocessing\n",
    "This dataset is clean, but we’ll verify and handle potential issues like outliers.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc978f3f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())  # Expect zeros\n",
    "\n",
    "# Check data types\n",
    "print(df.dtypes)  # Should all be float64\n",
    "\n",
    "# Visualize outliers with a box plot\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=df)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Cap outliers in CRIM\n",
    "df['CRIM'] = df['CRIM'].clip(upper=df['CRIM'].quantile(0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98cc4d2",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "**Missing Values:** None expected, but always check.\n",
    "\n",
    "**Data Types:** All numerical, no categorical encoding needed.\n",
    "\n",
    "**Outliers:** CRIM has extreme values; capping at the 99th percentile mitigates their impact.\n",
    "\n",
    "### Step 3: Exploratory Data Analysis (EDA)\n",
    "EDA reveals relationships between features and MEDV.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19d2367",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot: LSTAT vs MEDV\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df['LSTAT'], df['MEDV'], alpha=0.5)\n",
    "plt.title('LSTAT vs Median House Price')\n",
    "plt.xlabel('Lower Status Population (%)')\n",
    "plt.ylabel('Median House Price ($1000s)')\n",
    "plt.show()\n",
    "\n",
    "# Target distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(df['MEDV'], kde=True)\n",
    "plt.title('Distribution of Median House Prices')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e6490e",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "**Heatmap:** Shows correlations, e.g., RM (positive) and LSTAT (negative) with MEDV.\n",
    "\n",
    "**Scatter Plot:** LSTAT vs. MEDV shows a negative trend—higher LSTAT, lower prices.\n",
    "\n",
    "**Distribution:** MEDV is slightly right-skewed, good to know for modeling.\n",
    "\n",
    "### Step 4: Feature Engineering\n",
    "Enhance the dataset with new features and transformations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3846135",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# New feature: AGE squared\n",
    "df['AGE_squared'] = df['AGE'] ** 2\n",
    "\n",
    "# Log-transform CRIM\n",
    "df['log_CRIM'] = np.log1p(df['CRIM'])\n",
    "\n",
    "# Interaction term: RM * LSTAT\n",
    "df['RM_LSTAT'] = df['RM'] * df['LSTAT']\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_cols = df.columns.drop('MEDV')\n",
    "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a314ba",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "**New Feature:** AGE_squared captures non-linear effects of property age.\n",
    "\n",
    "**Log Transform:** Reduces skewness in CRIM.\n",
    "\n",
    "**Interaction:** RM_LSTAT combines room count and socioeconomic status.\n",
    "\n",
    "**Scaling:** Standardizes features for better model performance.\n",
    "\n",
    "### Step 5: Model Building\n",
    "Train multiple regression models and evaluate with RMSE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662b0c0a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Split data\n",
    "X = df.drop('MEDV', axis=1)\n",
    "y = df['MEDV']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr_pred = lr.predict(X_test)\n",
    "lr_rmse = np.sqrt(mean_squared_error(y_test, lr_pred))\n",
    "print(f\"Linear Regression RMSE: {lr_rmse:.4f}\")\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "print(f\"Random Forest RMSE: {rf_rmse:.4f}\")\n",
    "\n",
    "# Gradient Boosting\n",
    "gb = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "gb_pred = gb.predict(X_test)\n",
    "gb_rmse = np.sqrt(mean_squared_error(y_test, gb_pred))\n",
    "print(f\"Gradient Boosting RMSE: {gb_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ac1f3b",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "**Split:** 80% train, 20% test.\n",
    "\n",
    "**Models:** Linear Regression (baseline), Random Forest, and Gradient Boosting (ensemble methods).\n",
    "\n",
    "**RMSE:** Lower is better; ensemble models often outperform linear regression.\n",
    "\n",
    "## Step 6: Model Evaluation and Selection\n",
    "Use cross-validation to choose the best model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f7e7e1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Random Forest CV\n",
    "rf_cv_scores = cross_val_score(rf, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "rf_cv_rmse = np.sqrt(-rf_cv_scores.mean())\n",
    "print(f\"Random Forest CV RMSE: {rf_cv_rmse:.4f}\")\n",
    "\n",
    "# Gradient Boosting CV\n",
    "gb_cv_scores = cross_val_score(gb, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "gb_cv_rmse = np.sqrt(-gb_cv_scores.mean())\n",
    "print(f\"Gradient Boosting CV RMSE: {gb_cv_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8296ae1e",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "**Cross-Validation:** 5-fold CV averages performance across splits.\n",
    "\n",
    "**Selection:** Pick the model with the lowest CV RMSE (likely Gradient Boosting or Random Forest).\n",
    "\n",
    "## Step 7: Hyperparameter Tuning\n",
    "Optimize the best model (e.g., Gradient Boosting).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b97cd7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Grid Search\n",
    "grid = GridSearchCV(GradientBoostingRegressor(random_state=42), param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_gb = grid.best_estimator_\n",
    "best_pred = best_gb.predict(X_test)\n",
    "best_rmse = np.sqrt(mean_squared_error(y_test, best_pred))\n",
    "print(f\"Tuned Gradient Boosting RMSE: {best_rmse:.4f}\")\n",
    "print(f\"Best Parameters: {grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4fd324",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "**Grid Search:** Tests combinations of n_estimators, learning_rate, and max_depth.\n",
    "\n",
    "**Outcome:** Improved RMSE with the best parameters.\n",
    "\n",
    "## Step 8: Model Interpretability\n",
    "Understand feature impacts using SHAP.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749657a1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# SHAP values\n",
    "explainer = shap.Explainer(best_gb)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Summary plot\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5056741",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "***SHAP:*** Quantifies each feature’s contribution to predictions.\n",
    "\n",
    "**Key Features:** Likely RM, LSTAT, and engineered features like RM_LSTAT.\n",
    "\n",
    "## Step 9: Advanced Techniques\n",
    "Push further with XGBoost.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056d0e8c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_pred))\n",
    "print(f\"XGBoost RMSE: {xgb_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad02b4f2",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "**XGBoost:** A fast, powerful boosting algorithm.\n",
    "\n",
    "**Comparison:** Often outperforms Gradient Boosting slightly.\n",
    "\n",
    "## Step 10: Wrap Up\n",
    "Document your work and visualize results.\n",
    "# Code (Visualization)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f30b4d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Actual vs Predicted\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, best_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.title('Actual vs Predicted House Prices')\n",
    "plt.xlabel('Actual MEDV')\n",
    "plt.ylabel('Predicted MEDV')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aaa61b",
   "metadata": {},
   "source": [
    "## Report Outline\n",
    "***Introduction:*** Predicting Boston house prices.\n",
    "\n",
    "*Data:* Preprocessing and cleaning.\n",
    "\n",
    "*EDA:* Key insights with visuals.\n",
    "\n",
    "*Modeling:* Models, tuning, and performance.\n",
    "\n",
    "*Insights:* Top features from SHAP.\n",
    "\n",
    "*Conclusion:* Summary and next steps.\n",
    "\n",
    "### Final Tips\n",
    "*Iterate:* Revisit steps if needed.\n",
    "\n",
    "*Experiment:* Try different features or models.\n",
    "\n",
    "*Document:* Keep code clean and commented.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
